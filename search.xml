<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Pytorch 3: Neural Network</title>
      <link href="/post/3e370be8.html"/>
      <url>/post/3e370be8.html</url>
      
        <content type="html"><![CDATA[<p>Reference: <a href="https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html">https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html</a></p><h1 id="NN-Module"><a href="#NN-Module" class="headerlink" title="NN.Module"></a>NN.Module</h1><p>All neural networks inherit <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to">nn.Module</a></p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line">    </span><br><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><p>Although Pytorch does not have the built-in method like summary(model) in Tensorflow, there is an useful API called </p><p><a href="https://github.com/sksq96/pytorch-summary">torchsummary</a> which could do the job.</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html">nn.Flatten</a>: start_dim &#x3D; 1.Flattens a contiguous range of dims into a tensor.</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">nn.Linear</a>: randomly generate weight and bias to apply linear transformation on data from in_size to out_size</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html">nn.ReLU</a>: although this is a simple function, it is still a class.</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html">nn.Sequential</a>: order container of modules. There are other containers available.</p><h2 id="Buffer"><a href="#Buffer" class="headerlink" title="Buffer"></a>Buffer</h2><p>Not a model parameter but part of the module’s state</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">register_buffer</span>(<span class="params">self, name: <span class="built_in">str</span>, tensor: <span class="type">Optional</span>[Tensor], persistent: <span class="built_in">bool</span> = <span class="literal">True</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">       <span class="string">r&quot;&quot;&quot;Adds a buffer to the module.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">       This is typically used to register a buffer that should not to be</span></span><br><span class="line"><span class="string">       considered a model parameter. For example, BatchNorm&#x27;s ``running_mean``</span></span><br><span class="line"><span class="string">       is not a parameter, but is part of the module&#x27;s state. Buffers, by</span></span><br><span class="line"><span class="string">       default, are persistent and will be saved alongside parameters. This</span></span><br><span class="line"><span class="string">       behavior can be changed by setting :attr:`persistent` to ``False``. The</span></span><br><span class="line"><span class="string">       only difference between a persistent buffer and a non-persistent buffer</span></span><br><span class="line"><span class="string">       is that the latter will not be a part of this module&#x27;s</span></span><br><span class="line"><span class="string">       :attr:`state_dict`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">       Buffers can be accessed as attributes using given names.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">       Args:</span></span><br><span class="line"><span class="string">           name (string): name of the buffer. The buffer can be accessed</span></span><br><span class="line"><span class="string">               from this module using the given name</span></span><br><span class="line"><span class="string">           tensor (Tensor or None): buffer to be registered. If ``None``, then operations</span></span><br><span class="line"><span class="string">               that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,</span></span><br><span class="line"><span class="string">               the buffer is **not** included in the module&#x27;s :attr:`state_dict`.</span></span><br><span class="line"><span class="string">           persistent (bool): whether the buffer is part of this module&#x27;s</span></span><br><span class="line"><span class="string">               :attr:`state_dict`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">       Example::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">           &gt;&gt;&gt; self.register_buffer(&#x27;running_mean&#x27;, torch.zeros(num_features))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">       &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h2 id="Parameter"><a href="#Parameter" class="headerlink" title="Parameter"></a>Parameter</h2><p>Parameters are <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a> subclasses, that have a very special property when used with <code>Module</code> s - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in <code>parameters()</code> iterator. Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as <a href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter"><code>Parameter</code></a>, these temporaries would get registered too.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">register_parameter</span>(<span class="params">self, name: <span class="built_in">str</span>, param: <span class="type">Optional</span>[Parameter]</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Adds a parameter to the module.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The parameter can be accessed as an attribute using given name.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            name (string): name of the parameter. The parameter can be accessed</span></span><br><span class="line"><span class="string">                from this module using the given name</span></span><br><span class="line"><span class="string">            param (Parameter or None): parameter to be added to the module. If</span></span><br><span class="line"><span class="string">                ``None``, then operations that run on parameters, such as :attr:`cuda`,</span></span><br><span class="line"><span class="string">                are ignored. If ``None``, the parameter is **not** included in the</span></span><br><span class="line"><span class="string">                module&#x27;s :attr:`state_dict`.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h2 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_module</span>(<span class="params">self, name: <span class="built_in">str</span>, module: <span class="type">Optional</span>[<span class="string">&#x27;Module&#x27;</span>]</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Adds a child module to the current module.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The module can be accessed as an attribute using the given name.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            name (string): name of the child module. The child module can be</span></span><br><span class="line"><span class="string">                accessed from this module using the given name</span></span><br><span class="line"><span class="string">            module (Module): child module to be added to the module.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch 2: Custom Dataset into DataLoader</title>
      <link href="/post/e6185ae.html"/>
      <url>/post/e6185ae.html</url>
      
        <content type="html"><![CDATA[<p>Reference: <a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">https://pytorch.org/tutorials/beginner/basics/data_tutorial.html</a></p><h1 id="Custom-Dataset"><a href="#Custom-Dataset" class="headerlink" title="Custom Dataset"></a>Custom Dataset</h1><p>Three must-implemented functions: <strong>init</strong>, <strong>len</strong>, and <strong>getitem</strong>.</p><p>Pytorch supports 2 types of dataset:</p><ul><li><p>Map-style dataset: </p><p>A map-style dataset is one that implements the <code>__getitem__()</code> and <code>__len__()</code> protocols, and represents a map from (possibly non-integral) indices&#x2F;keys to data samples.</p><p>For example, such a dataset, when accessed with <code>dataset[idx]</code>, could read the <code>idx</code>-th image and its corresponding label from a folder on the disk.</p></li><li><p>Iterable-style dataset:</p><p>An iterable-style dataset is an instance of a subclass of <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset"><code>IterableDataset</code></a> that implements the <code>__iter__()</code> protocol, and represents an iterable over data samples. This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data.</p><p>For example, such a dataset, when called <code>iter(dataset)</code>, could return a stream of data reading from a database, a remote server, or even logs generated in real time.</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torchvision.io <span class="keyword">import</span> read_image</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomImageDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span><br><span class="line">        self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">        self.img_dir = img_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        label = self.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">if</span> self.target_transform:</span><br><span class="line">            label = self.target_transform(label)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><h1 id="DataLoader-Implementation"><a href="#DataLoader-Implementation" class="headerlink" title="DataLoader Implementation"></a>DataLoader Implementation</h1><p>Simple implementation of Dataloader on Dataset:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display image and label.</span></span><br><span class="line">train_features, train_labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Feature batch shape: <span class="subst">&#123;train_features.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Labels batch shape: <span class="subst">&#123;train_labels.size()&#125;</span>&quot;</span>)</span><br><span class="line">img = train_features[<span class="number">0</span>].squeeze()</span><br><span class="line">label = train_labels[<span class="number">0</span>]</span><br><span class="line">plt.imshow(img, cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Label: <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">Feature batch shape: torch.Size([<span class="number">64</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">Labels batch shape: torch.Size([<span class="number">64</span>])</span><br><span class="line">Label: <span class="number">6</span></span><br></pre></td></tr></table></figure><h1 id="DataLoader-Original-Code-Analysis"><a href="#DataLoader-Original-Code-Analysis" class="headerlink" title="DataLoader Original Code Analysis"></a>DataLoader Original Code Analysis</h1><h2 id="Constructor"><a href="#Constructor" class="headerlink" title="Constructor:"></a>Constructor:</h2><p>When to write your own sampler or batch sampler?</p><ul><li>You can write a customized sampler when you need to samples with specific features. For example, you want sample data with similar length to be within a batch. </li><li>When you have your own sampler, set shuffle as false.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">dataset: Dataset[T_co] <span class="comment"># your custom dataset with __init__, __len__, and __getitem__</span></span><br><span class="line">batch_size: <span class="type">Optional</span>[<span class="built_in">int</span>]  </span><br><span class="line">num_workers: <span class="built_in">int</span></span><br><span class="line">pin_memory: <span class="built_in">bool</span></span><br><span class="line">drop_last: <span class="built_in">bool</span></span><br><span class="line">timeout: <span class="built_in">float</span></span><br><span class="line">sampler: <span class="type">Union</span>[Sampler, Iterable]</span><br><span class="line">pin_memory_device: <span class="built_in">str</span></span><br><span class="line">prefetch_factor: <span class="built_in">int</span></span><br><span class="line">_iterator : <span class="type">Optional</span>[<span class="string">&#x27;_BaseDataLoaderIter&#x27;</span>]</span><br><span class="line">__initialized = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset: Dataset[T_co], batch_size: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">             shuffle: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>, <span class="comment"># if shuffle after each epoch</span></span></span><br><span class="line"><span class="params">             sampler: <span class="type">Union</span>[Sampler, Iterable, <span class="literal">None</span>] = <span class="literal">None</span>, <span class="comment"># decide how to sample the dataset, u can customize it</span></span></span><br><span class="line"><span class="params">             batch_sampler: <span class="type">Union</span>[Sampler[<span class="type">Sequence</span>], Iterable[<span class="type">Sequence</span>], <span class="literal">None</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">             num_workers: <span class="built_in">int</span> = <span class="number">0</span>, <span class="comment"># based on cpu numbers</span></span></span><br><span class="line"><span class="params">             collate_fn: <span class="type">Optional</span>[_collate_fn_t] = <span class="literal">None</span>, <span class="comment"># function to process each batch after sampling, customize it</span></span></span><br><span class="line"><span class="params">             pin_memory: <span class="built_in">bool</span> = <span class="literal">False</span>, <span class="comment"># if save tensor in gpu</span></span></span><br><span class="line"><span class="params">             drop_last: <span class="built_in">bool</span> = <span class="literal">False</span>, <span class="comment"># drop last batch that is smaller than batch size</span></span></span><br><span class="line"><span class="params">             timeout: <span class="built_in">float</span> = <span class="number">0</span>, worker_init_fn: <span class="type">Optional</span>[_worker_init_fn_t] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">             multiprocessing_context=<span class="literal">None</span>, generator=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">             *, prefetch_factor: <span class="built_in">int</span> = <span class="number">2</span>,</span></span><br><span class="line"><span class="params">             persistent_workers: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">             pin_memory_device: <span class="built_in">str</span> = <span class="string">&quot;&quot;</span></span>):</span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>if have sampler, make sure shuffle is false</li><li>if have batch sampler, no need to set batch size, shuffle, sampler, or drop_last   </li><li>if no batch size specified, make sure drop_last false</li><li>if shuffle, use RandomSampler, else use SequentialSampler</li><li>if no collate_fn, then if have batch sampler, then use default_collate(do nothing), else use default_convert</li></ul><p>What does collate_fn do?</p><ul><li>For example, you have 4 sentences in a mini-batch. You need to do the padding on 3 shorter sentences. Then, you can write a collate_fn that get the longest sentence’s length and pad the other 3 sentences so that all of them have the same length.</li></ul><h2 id="Iteration"><a href="#Iteration" class="headerlink" title="Iteration"></a>Iteration</h2><p>We usually use iter(DataLoader) to do the batch iterations. When num_workers &#x3D;&#x3D; 0, use SingleProcessDataLoaderIter to return an iterator; when &gt;0, use MultiProcessDataLoaderIter to return an iterator. In the iterator, the father class BaseDataLoaderIter can respond to next() function to return a mini-batch every time.</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch I: Common Tensor Operations</title>
      <link href="/post/ae04ef98.html"/>
      <url>/post/ae04ef98.html</url>
      
        <content type="html"><![CDATA[<p>Reference: <a href="https://pytorch.org/docs/stable/torch.html">https://pytorch.org/docs/stable/torch.html</a>, <a href="https://pytorch.org/docs/stable/tensor_attributes.html">https://pytorch.org/docs/stable/tensor_attributes.html</a></p><h1 id="Tensor-Attributes"><a href="#Tensor-Attributes" class="headerlink" title="Tensor Attributes"></a>Tensor Attributes</h1><p>Tensor has 3 basic attributes: <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype"><code>torch.dtype</code></a>, <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device"><code>torch.device</code></a>, and <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.layout"><code>torch.layout</code></a>.</p><p>Note: There are two 16-bit floating point data type. </p><p><strong>torch.float16</strong> referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important.</p><p><strong>torch.bfloat16</strong> referred to as Brain Floating Point: use 1 sign, 8 exponent and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as <code>float32</code></p><h1 id="Tensor-Common-Operations-torch"><a href="#Tensor-Common-Operations-torch" class="headerlink" title="Tensor Common Operations (torch.)"></a>Tensor Common Operations (torch.)</h1><h2 id="Tensor-Properties"><a href="#Tensor-Properties" class="headerlink" title="Tensor Properties"></a>Tensor Properties</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor"><code>is_tensor</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.is_complex.html#torch.is_complex"><code>is_complex</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.is_conj.html#torch.is_conj"><code>is_conj</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel"><code>numel</code></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">0.4584</span>, <span class="number">0.6125</span>, <span class="number">0.5969</span>, <span class="number">0.8006</span>],</span><br><span class="line">        [<span class="number">0.7024</span>, <span class="number">0.9120</span>, <span class="number">0.3130</span>, <span class="number">0.2433</span>],</span><br><span class="line">        [<span class="number">0.7482</span>, <span class="number">0.5848</span>, <span class="number">0.4191</span>, <span class="number">0.9261</span>],</span><br><span class="line">        [<span class="number">0.9341</span>, <span class="number">0.1130</span>, <span class="number">0.2907</span>, <span class="number">0.4792</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.is_tensor(a)</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.is_complex(a)</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.is_conj(a)</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.numel(a) <span class="comment"># total elements in the tensor</span></span><br><span class="line"><span class="number">16</span></span><br></pre></td></tr></table></figure><h2 id="Tensor-Creation"><a href="#Tensor-Creation" class="headerlink" title="Tensor Creation"></a>Tensor Creation</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.asarray.html#torch.asarray"><code>asarray</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.as_tensor.html#torch.as_tensor"><code>as_tensor</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros"><code>zeros</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like"><code>ones_like</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange"><code>arange</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"><code>linspace</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"><code>logspace</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"><code>eye</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty"><code>empty</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"><code>full</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"><code>complex</code></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.asarray([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.as_tensor([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones_like(a)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">12</span>)</span><br><span class="line">tensor([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">3</span>,<span class="number">13</span>,<span class="number">4</span>) <span class="comment">#start, end(exclude), step_size</span></span><br><span class="line">tensor([ <span class="number">3</span>,  <span class="number">7</span>, <span class="number">11</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.linspace(-<span class="number">5</span>,<span class="number">10</span>, <span class="number">8</span>) <span class="comment">#start, end, size</span></span><br><span class="line">tensor([-<span class="number">5.0000</span>, -<span class="number">2.8571</span>, -<span class="number">0.7143</span>,  <span class="number">1.4286</span>,  <span class="number">3.5714</span>,  <span class="number">5.7143</span>,  <span class="number">7.8571</span>, <span class="number">10.0000</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.logspace(-<span class="number">5</span>, <span class="number">10</span>, <span class="number">4</span>, <span class="number">2</span>) <span class="comment">#start, end, size, base</span></span><br><span class="line">tensor([<span class="number">3.1250e-02</span>, <span class="number">1.0000e+00</span>, <span class="number">3.2000e+01</span>, <span class="number">1.0240e+03</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.eye(<span class="number">3</span>)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.empty([<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">tensor([[<span class="number">1.4013e-45</span>, <span class="number">0.0000e+00</span>],</span><br><span class="line">        [<span class="number">0.0000e+00</span>, <span class="number">0.0000e+00</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.full([<span class="number">2</span>,<span class="number">2</span>], <span class="number">7</span>)</span><br><span class="line">tensor([[<span class="number">7</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">7</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.<span class="built_in">complex</span>(torch.tensor([<span class="number">1.</span>,<span class="number">2.</span>]), torch.tensor([<span class="number">3.</span>,<span class="number">4.</span>])) <span class="comment">#Expected both inputs to be Half, Float or Double tensors</span></span><br><span class="line">tensor([<span class="number">1.</span>+<span class="number">3.j</span>, <span class="number">2.</span>+<span class="number">4.j</span>])</span><br></pre></td></tr></table></figure><h2 id="Tensor-Indexing-Slicing-Joining-Mutating"><a href="#Tensor-Indexing-Slicing-Joining-Mutating" class="headerlink" title="Tensor Indexing, Slicing, Joining, Mutating"></a>Tensor Indexing, Slicing, Joining, Mutating</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat"><code>cat</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk"><code>chunk</code></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.rand(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.rand(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((a,b), <span class="number">0</span>)</span><br><span class="line">tensor([[<span class="number">0.2232</span>, <span class="number">0.0491</span>],</span><br><span class="line">        [<span class="number">0.2638</span>, <span class="number">0.2779</span>],</span><br><span class="line">        [<span class="number">0.0076</span>, <span class="number">0.3428</span>],</span><br><span class="line">        [<span class="number">0.1500</span>, <span class="number">0.4123</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((a,b), <span class="number">1</span>)</span><br><span class="line">tensor([[<span class="number">0.2232</span>, <span class="number">0.0491</span>, <span class="number">0.0076</span>, <span class="number">0.3428</span>],</span><br><span class="line">        [<span class="number">0.2638</span>, <span class="number">0.2779</span>, <span class="number">0.1500</span>, <span class="number">0.4123</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">16</span>).reshape([<span class="number">4</span>,<span class="number">4</span>]).chunk(<span class="number">4</span>, <span class="number">0</span>) <span class="comment"># input, chunks, dim</span></span><br><span class="line">(tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]), tensor([[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]]), tensor([[ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]]), tensor([[<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">16</span>).reshape([<span class="number">4</span>,<span class="number">4</span>]).chunk(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">(tensor([[ <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">4</span>],</span><br><span class="line">        [ <span class="number">8</span>],</span><br><span class="line">        [<span class="number">12</span>]]), tensor([[ <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">9</span>],</span><br><span class="line">        [<span class="number">13</span>]]), tensor([[ <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">6</span>],</span><br><span class="line">        [<span class="number">10</span>],</span><br><span class="line">        [<span class="number">14</span>]]), tensor([[ <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">7</span>],</span><br><span class="line">        [<span class="number">11</span>],</span><br><span class="line">        [<span class="number">15</span>]]))</span><br></pre></td></tr></table></figure><p><a href="https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather"><code>gather</code></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">out[i][j][k] = <span class="built_in">input</span>[index[i][j][k]][j][k]  <span class="comment"># if dim == 0</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][index[i][j][k]][k]  <span class="comment"># if dim == 1</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][j][index[i][j][k]]  <span class="comment"># if dim == 2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.gather(t, <span class="number">1</span>, torch.tensor([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>]]))</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">3</span>]])</span><br></pre></td></tr></table></figure><p><a href="https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"><code>narrow</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.nonzero.html#torch.nonzero"><code>nonzero</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.permute.html#torch.permute"><code>permute</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape"><code>reshape</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"><code>split</code></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">4</span>,<span class="number">4</span>).narrow(<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>).shape <span class="comment"># input, dim, start index, length remaining</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">4</span>,<span class="number">4</span>).narrow(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.eye(<span class="number">2</span>).nonzero() <span class="comment"># return indices of non_zero elements</span></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.permute(x, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)).size()</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">4</span>,<span class="number">4</span>).reshape(-<span class="number">1</span>, <span class="number">2</span>).shape</span><br><span class="line">torch.Size([<span class="number">8</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">4</span>,<span class="number">4</span>).split(<span class="number">2</span>) <span class="comment">#input, chunks</span></span><br><span class="line">(tensor([[<span class="number">0.1341</span>, <span class="number">0.7247</span>, <span class="number">0.4264</span>, <span class="number">0.7144</span>],</span><br><span class="line">        [<span class="number">0.4725</span>, <span class="number">0.2130</span>, <span class="number">0.0141</span>, <span class="number">0.1115</span>]]), tensor([[<span class="number">0.6829</span>, <span class="number">0.3465</span>, <span class="number">0.4018</span>, <span class="number">0.4399</span>],</span><br><span class="line">        [<span class="number">0.9782</span>, <span class="number">0.0578</span>, <span class="number">0.3783</span>, <span class="number">0.1574</span>]]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">4</span>,<span class="number">4</span>).split([<span class="number">1</span>,<span class="number">3</span>]) <span class="comment">#input, list of sizes</span></span><br><span class="line">(tensor([[<span class="number">0.4803</span>, <span class="number">0.0487</span>, <span class="number">0.2502</span>, <span class="number">0.7391</span>]]), tensor([[<span class="number">0.0844</span>, <span class="number">0.1045</span>, <span class="number">0.7599</span>, <span class="number">0.1434</span>],</span><br><span class="line">        [<span class="number">0.1662</span>, <span class="number">0.9619</span>, <span class="number">0.4036</span>, <span class="number">0.5674</span>],</span><br><span class="line">        [<span class="number">0.4526</span>, <span class="number">0.2717</span>, <span class="number">0.5808</span>, <span class="number">0.7450</span>]]))</span><br></pre></td></tr></table></figure><p><a href="https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"><code>squeeze</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.stack.html#torch.stack"><code>stack</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.take.html#torch.take"><code>take</code></a>,<a href="https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"><code>tile</code></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand([<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">6</span>]).squeeze().shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack((torch.rand(<span class="number">3</span>,<span class="number">5</span>), torch.rand(<span class="number">3</span>,<span class="number">5</span>)), <span class="number">0</span>).shape <span class="comment"># tensors, dim</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack((torch.rand(<span class="number">3</span>,<span class="number">5</span>), torch.rand(<span class="number">3</span>,<span class="number">5</span>)), <span class="number">1</span>).shape <span class="comment"># concat along a new dim indicated</span></span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack((torch.rand(<span class="number">3</span>,<span class="number">5</span>), torch.rand(<span class="number">3</span>,<span class="number">5</span>)), <span class="number">2</span>).shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">10</span>).take(torch.tensor([<span class="number">0</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">8</span>])) <span class="comment">#treat input as 1-D, take elements on these indices</span></span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.tile((<span class="number">2</span>,))</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tile(y, (<span class="number">2</span>, <span class="number">2</span>)) <span class="comment"># input, the number of repetitions per dimension</span></span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure><p><a href="https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose"><code>transpose</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind"><code>unbind</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze"><code>unsqueeze</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.where.html#torch.where"><code>where</code></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">1.0028</span>, -<span class="number">0.9893</span>,  <span class="number">0.5809</span>],</span><br><span class="line">        [-<span class="number">0.1669</span>,  <span class="number">0.7299</span>,  <span class="number">0.4942</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.transpose(x, <span class="number">0</span>, <span class="number">1</span>) <span class="comment"># input, dim0, dim1; exchange dim0 and dim1</span></span><br><span class="line">tensor([[ <span class="number">1.0028</span>, -<span class="number">0.1669</span>],</span><br><span class="line">        [-<span class="number">0.9893</span>,  <span class="number">0.7299</span>],</span><br><span class="line">        [ <span class="number">0.5809</span>,  <span class="number">0.4942</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>).unbind(<span class="number">1</span>) <span class="comment">#input, dim to be removed</span></span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">3</span>, <span class="number">6</span>]), tensor([<span class="number">1</span>, <span class="number">4</span>, <span class="number">7</span>]), tensor([<span class="number">2</span>, <span class="number">5</span>, <span class="number">8</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>).unbind(<span class="number">0</span>)</span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]), tensor([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]), tensor([<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">4</span>,<span class="number">5</span>).unsqueeze(<span class="number">0</span>).shape <span class="comment">#input, dim</span></span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">4</span>,<span class="number">5</span>).unsqueeze(<span class="number">1</span>).shape <span class="comment"># insert a dim of size one</span></span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">4</span>,<span class="number">5</span>).unsqueeze(<span class="number">2</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.ones(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[-<span class="number">0.4620</span>,  <span class="number">0.3139</span>],</span><br><span class="line">        [ <span class="number">0.3898</span>, -<span class="number">0.7197</span>],</span><br><span class="line">        [ <span class="number">0.0478</span>, -<span class="number">0.1657</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.where(x &gt; <span class="number">0</span>, x, y) <span class="comment"># if &gt;0,x, else y</span></span><br><span class="line">tensor([[ <span class="number">1.0000</span>,  <span class="number">0.3139</span>],</span><br><span class="line">        [ <span class="number">0.3898</span>,  <span class="number">1.0000</span>],</span><br><span class="line">        [ <span class="number">0.0478</span>,  <span class="number">1.0000</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">2</span>, dtype=torch.double)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">1.0779</span>,  <span class="number">0.0383</span>],</span><br><span class="line">        [-<span class="number">0.8785</span>, -<span class="number">1.1089</span>]], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.where(x &gt; <span class="number">0</span>, x, <span class="number">0.</span>) <span class="comment">#if &gt;0, x, else 0.0</span></span><br><span class="line">tensor([[<span class="number">1.0779</span>, <span class="number">0.0383</span>],</span><br><span class="line">        [<span class="number">0.0000</span>, <span class="number">0.0000</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure><h2 id="Tensor-Sampling"><a href="#Tensor-Sampling" class="headerlink" title="Tensor Sampling"></a>Tensor Sampling</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed"><code>manual_seed</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.bernoulli.html#torch.bernoulli"><code>bernoulli</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal"><code>normal</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"><code>randint</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn"><code>randn</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.randperm.html#torch.randperm"><code>randperm</code></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="number">142</span>)</span><br><span class="line">&lt;torch._C.Generator <span class="built_in">object</span> at <span class="number">0x0000023A3B71DF50</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">3</span>,<span class="number">3</span>).bernoulli() <span class="comment"># each element means probability of getting 1</span></span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.normal(mean=torch.arange(<span class="number">1.</span>, <span class="number">11.</span>), std=torch.arange(<span class="number">1</span>, <span class="number">0</span>, -<span class="number">0.1</span>)) <span class="comment"># len(mean) = len(std) one sample per index</span></span><br><span class="line">tensor([  <span class="number">1.0425</span>,   <span class="number">3.5672</span>,   <span class="number">2.7969</span>,   <span class="number">4.2925</span>,   <span class="number">4.7229</span>,   <span class="number">6.2134</span>,</span><br><span class="line">          <span class="number">8.0505</span>,   <span class="number">8.1408</span>,   <span class="number">9.0563</span>,  <span class="number">10.0566</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.normal(mean=<span class="number">0.5</span>, std=torch.arange(<span class="number">1.</span>, <span class="number">6.</span>)) <span class="comment"># single mean, multi-std, one smaple per std</span></span><br><span class="line">tensor([-<span class="number">1.2793</span>, -<span class="number">1.0732</span>, -<span class="number">2.0687</span>,  <span class="number">5.1177</span>, -<span class="number">1.2303</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.normal(mean=torch.arange(<span class="number">1.</span>, <span class="number">6.</span>)) <span class="comment"># std default 1.0, one sample per mean</span></span><br><span class="line">tensor([ <span class="number">1.1552</span>,  <span class="number">2.6148</span>,  <span class="number">2.6535</span>,  <span class="number">5.8318</span>,  <span class="number">4.2361</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.normal(<span class="number">2</span>,<span class="number">3</span>,(<span class="number">2</span>,<span class="number">4</span>)) <span class="comment"># mean, std, shape</span></span><br><span class="line">tensor([[ <span class="number">6.1841</span>, -<span class="number">0.9751</span>, -<span class="number">2.5606</span>,  <span class="number">4.0752</span>],</span><br><span class="line">        [ <span class="number">2.6387</span>,  <span class="number">2.0165</span>,  <span class="number">1.8390</span>, -<span class="number">0.3671</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randint(<span class="number">3</span>, <span class="number">10</span>, (<span class="number">2</span>, <span class="number">2</span>)) <span class="comment">#low =0, high, shape</span></span><br><span class="line">tensor([[<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randperm(<span class="number">10</span>)</span><br><span class="line">tensor([<span class="number">9</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><ul><li><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_"><code>torch.Tensor.bernoulli_()</code></a> - in-place version of <a href="https://pytorch.org/docs/stable/generated/torch.bernoulli.html#torch.bernoulli"><code>torch.bernoulli()</code></a></li><li><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_"><code>torch.Tensor.cauchy_()</code></a> - numbers drawn from the Cauchy distribution</li><li><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_"><code>torch.Tensor.exponential_()</code></a> - numbers drawn from the exponential distribution</li><li><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_"><code>torch.Tensor.geometric_()</code></a> - elements drawn from the geometric distribution</li><li><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_"><code>torch.Tensor.log_normal_()</code></a> - samples from the log-normal distribution</li><li><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"><code>torch.Tensor.normal_()</code></a> - in-place version of <a href="https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal"><code>torch.normal()</code></a></li><li><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.random_.html#torch.Tensor.random_"><code>torch.Tensor.random_()</code></a> - numbers sampled from the discrete uniform distribution</li><li><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"><code>torch.Tensor.uniform_()</code></a> - numbers sampled from the continuous uniform distribution</li></ul><h1 id="Arithmetic-Operations"><a href="#Arithmetic-Operations" class="headerlink" title="Arithmetic Operations"></a>Arithmetic Operations</h1><p>+-*&#x2F;, <a href="https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow"><code>pow</code></a>, @ (for matrix multiplication)</p><h1 id="More-in-the-reference"><a href="#More-in-the-reference" class="headerlink" title="More in the reference"></a>More in the reference</h1>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Future Plan of My Blog</title>
      <link href="/post/ed40dce2.html"/>
      <url>/post/ed40dce2.html</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This is the first post published on this website. For the next period of time, this site will be <strong>my official personal website as well as the blog</strong> where I may dump whatever shit I get, including but not limited to my diary, reading notes, movies recommendation, and shitty greentexts (perhaps). Since Hexo is a statistically-generated frame of blog websites, I did not expect to include the comment or discussion sections at the beginning. However, it seems that Butterfly, the theme of this website, does have a way to let visitors leave comments by establishing the connections with some external services. Thus, I plan to open the discussion section soon so that visitors can leave their thoughts (if there is any). All in all, I hope I could <strong>keep this positivity and continually bring new stuff onto this blog</strong>. </p><h1 id="Why-Blog"><a href="#Why-Blog" class="headerlink" title="Why Blog?"></a>Why Blog?</h1><p>I always believe that <strong><em>people value their own properties more than those are given</em>.</strong> I did consider of recording my posts on some social media platforms, and I say no. Why do I have to comply with the rules of those platforms when I can have my own system? Moreover, because this is the website I built from nothing (although no big setback was encountered), I value it more than my other social media accounts. Thus, I think this blog will be the best place to track my thoughts and progress.</p><h1 id="Who-are-Welcomed"><a href="#Who-are-Welcomed" class="headerlink" title="Who are Welcomed?"></a>Who are Welcomed?</h1><p>Every friendly visitor who does not break the rules of discussion.</p><h1 id="Prohibitions"><a href="#Prohibitions" class="headerlink" title="Prohibitions"></a>Prohibitions</h1><ul><li>No discussion of <strong>Politics</strong> </li><li>No <strong>Profanities</strong> </li><li>No <strong>Pornographies</strong></li><li>No <strong>Spamming</strong></li><li>No <strong>Ads</strong></li></ul><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>EDG在令人失望这方面真是从来没令人失望。</p>]]></content>
      
      
      <categories>
          
          <category> Diary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Daily </tag>
            
            <tag> Game </tag>
            
            <tag> Genshin Impact </tag>
            
            <tag> OTTO </tag>
            
            <tag> Anime </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>About Me</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<p>This is me.</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Blog Tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Useful Links</title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>discussion</title>
      <link href="/discussion/index.html"/>
      <url>/discussion/index.html</url>
      
        <content type="html"><![CDATA[<p>Leave Your Thoughts Here.</p>]]></content>
      
    </entry>
    
    
  
</search>
